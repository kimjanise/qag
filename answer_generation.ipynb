{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZwDHoU5ED7g9"
   },
   "outputs": [],
   "source": [
    "!pip install transformers torch sentencepiece datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_eixCXJRGEVv"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "QA_EPOCHS = 3\n",
    "BOOL_EPOCHS = 3\n",
    "QA_LR = 3e-5\n",
    "BOOL_LR = 2e-5\n",
    "MAX_INPUT_LEN = 512\n",
    "MAX_OUTPUT_LEN = 32\n",
    "MAX_TRAIN_SAMPLES = 10000  # subsample for Colab feasibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "531ab155"
   },
   "outputs": [],
   "source": [
    "squad = load_dataset(\"squad_v2\")\n",
    "print(squad)\n",
    "print(f\"Train examples: {len(squad['train'])}\")\n",
    "print(f\"Validation examples: {len(squad['validation'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune T5-Base (Extractive QA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iM78uiXcLPRu"
   },
   "outputs": [],
   "source": [
    "qa_tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "qa_model_ft = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "\n",
    "train_data = squad[\"train\"].shuffle(seed=SEED).select(range(min(MAX_TRAIN_SAMPLES, len(squad[\"train\"]))))\n",
    "val_data = squad[\"validation\"].shuffle(seed=SEED).select(range(min(MAX_TRAIN_SAMPLES // 5, len(squad[\"validation\"]))))\n",
    "\n",
    "def preprocess_qa(examples):\n",
    "    inputs = [\n",
    "        f\"question: {q}  context: {c}\"\n",
    "        for q, c in zip(examples[\"question\"], examples[\"context\"])\n",
    "    ]\n",
    "    targets = [\n",
    "        ans[\"text\"][0] if len(ans[\"text\"]) > 0 else \"\"\n",
    "        for ans in examples[\"answers\"]\n",
    "    ]\n",
    "\n",
    "    model_inputs = qa_tokenizer(\n",
    "        inputs, max_length=MAX_INPUT_LEN, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "    labels = qa_tokenizer(\n",
    "        targets, max_length=MAX_OUTPUT_LEN, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "    # Replace pad token ids with -100 so they are ignored in loss\n",
    "    label_ids = [\n",
    "        [(tok if tok != qa_tokenizer.pad_token_id else -100) for tok in label]\n",
    "        for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "    model_inputs[\"labels\"] = label_ids\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "train_dataset = train_data.map(preprocess_qa, batched=True, remove_columns=train_data.column_names)\n",
    "val_dataset = val_data.map(preprocess_qa, batched=True, remove_columns=val_data.column_names)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./t5_base_squad2_checkpoints\",\n",
    "    num_train_epochs=QA_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=QA_LR,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    seed=SEED,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=qa_model_ft,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "print(\"Starting T5-Base fine-tuning on SQuAD v2...\")\n",
    "trainer.train()\n",
    "\n",
    "qa_model_ft.save_pretrained(\"./t5_base_squad2/\")\n",
    "qa_tokenizer.save_pretrained(\"./t5_base_squad2/\")\n",
    "print(\"T5-Base saved to ./t5_base_squad2/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune RoBERTa-Base (Boolean Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answerable = [ex for ex in squad[\"train\"] if len(ex[\"answers\"][\"text\"]) > 0]\n",
    "all_contexts = [ex[\"context\"] for ex in answerable]\n",
    "print(f\"Answerable examples: {len(answerable)}\")\n",
    "\n",
    "true_pairs = []\n",
    "false_pairs = []\n",
    "\n",
    "sampled = answerable[:min(MAX_TRAIN_SAMPLES, len(answerable))]\n",
    "\n",
    "for ex in sampled:\n",
    "    question = ex[\"question\"]\n",
    "    correct_context = ex[\"context\"]\n",
    "\n",
    "    true_pairs.append({\"question\": question, \"passage\": correct_context, \"label\": 1})\n",
    "\n",
    "    wrong_context = correct_context\n",
    "    while wrong_context == correct_context:\n",
    "        wrong_context = random.choice(all_contexts)\n",
    "    false_pairs.append({\"question\": question, \"passage\": wrong_context, \"label\": 0})\n",
    "\n",
    "bool_data = true_pairs + false_pairs\n",
    "random.shuffle(bool_data)\n",
    "\n",
    "# Split 90/10 train/val\n",
    "split_idx = int(len(bool_data) * 0.9)\n",
    "bool_train = Dataset.from_list(bool_data[:split_idx])\n",
    "bool_val = Dataset.from_list(bool_data[split_idx:])\n",
    "\n",
    "print(f\"Boolean train size: {len(bool_train)}\")\n",
    "print(f\"Boolean val size: {len(bool_val)}\")\n",
    "print(f\"Label distribution (train): {sum(1 for x in bool_data[:split_idx] if x['label']==1)} true, \"\n",
    "      f\"{sum(1 for x in bool_data[:split_idx] if x['label']==0)} false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "bool_model_ft = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "\n",
    "\n",
    "def preprocess_bool(examples):\n",
    "    return bool_tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"passage\"],\n",
    "        max_length=MAX_INPUT_LEN,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "\n",
    "bool_train_tok = bool_train.map(preprocess_bool, batched=True, remove_columns=[\"question\", \"passage\"])\n",
    "bool_val_tok = bool_val.map(preprocess_bool, batched=True, remove_columns=[\"question\", \"passage\"])\n",
    "\n",
    "bool_training_args = TrainingArguments(\n",
    "    output_dir=\"./roberta_base_boolq_checkpoints\",\n",
    "    num_train_epochs=BOOL_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=BOOL_LR,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    seed=SEED,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "bool_trainer = Trainer(\n",
    "    model=bool_model_ft,\n",
    "    args=bool_training_args,\n",
    "    train_dataset=bool_train_tok,\n",
    "    eval_dataset=bool_val_tok,\n",
    "    data_collator=DataCollatorWithPadding(bool_tokenizer),\n",
    ")\n",
    "\n",
    "print(\"Starting RoBERTa-Base fine-tuning on boolean pairs...\")\n",
    "bool_trainer.train()\n",
    "\n",
    "bool_model_ft.save_pretrained(\"./roberta_base_boolq/\")\n",
    "bool_tokenizer.save_pretrained(\"./roberta_base_boolq/\")\n",
    "print(\"RoBERTa-Base saved to ./roberta_base_boolq/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_model_path = './t5_base_squad2/'\n",
    "qa_tokenizer_path = './t5_base_squad2/'\n",
    "bool_model_path = './roberta_base_boolq/'\n",
    "bool_tokenizer_path = './roberta_base_boolq/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_model = T5ForConditionalGeneration.from_pretrained(qa_model_path).to(DEVICE)\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(qa_tokenizer_path)\n",
    "\n",
    "bool_model = AutoModelForSequenceClassification.from_pretrained(bool_model_path).to(DEVICE)\n",
    "bool_tokenizer = AutoTokenizer.from_pretrained(bool_tokenizer_path)\n",
    "\n",
    "qa_model.eval()\n",
    "bool_model.eval()\n",
    "print(\"Fine-tuned models loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(question, context, model=qa_model, tokenizer=qa_tokenizer):\n",
    "    input_text = f\"question: {question}  context: {context}\"\n",
    "    encoding = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=MAX_INPUT_LEN)\n",
    "    input_ids = encoding[\"input_ids\"].to(DEVICE)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=MAX_OUTPUT_LEN)\n",
    "    answer = tokenizer.decode(output[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    return answer.strip().capitalize()\n",
    "\n",
    "\n",
    "def beam_search_decoding(question, context, model=qa_model, tokenizer=qa_tokenizer):\n",
    "    input_text = f\"question: {question}  context: {context}\"\n",
    "    encoding = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=MAX_INPUT_LEN)\n",
    "    input_ids = encoding[\"input_ids\"].to(DEVICE)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=MAX_OUTPUT_LEN,\n",
    "            num_beams=10,\n",
    "            num_return_sequences=3,\n",
    "            no_repeat_ngram_size=2,\n",
    "            early_stopping=True,\n",
    "        )\n",
    "    answers = [tokenizer.decode(out, skip_special_tokens=True, clean_up_tokenization_spaces=True) for out in output]\n",
    "    return [a.strip().capitalize() for a in answers]\n",
    "\n",
    "\n",
    "def topkp_decoding(question, context, model=qa_model, tokenizer=qa_tokenizer):\n",
    "    input_text = f\"question: {question}  context: {context}\"\n",
    "    encoding = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=MAX_INPUT_LEN)\n",
    "    input_ids = encoding[\"input_ids\"].to(DEVICE)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=MAX_OUTPUT_LEN,\n",
    "            do_sample=True,\n",
    "            top_k=40,\n",
    "            top_p=0.80,\n",
    "            num_return_sequences=3,\n",
    "            no_repeat_ngram_size=2,\n",
    "            early_stopping=True,\n",
    "        )\n",
    "    answers = [tokenizer.decode(out, skip_special_tokens=True, clean_up_tokenization_spaces=True) for out in output]\n",
    "    return [a.strip().capitalize() for a in answers]\n",
    "\n",
    "\n",
    "def classify_true_false(question, passage, model=bool_model, tokenizer=bool_tokenizer):\n",
    "    inputs = tokenizer(question, passage, return_tensors=\"pt\", truncation=True, max_length=MAX_INPUT_LEN).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    probs = torch.softmax(logits, dim=-1)[0]\n",
    "    label = torch.argmax(probs).item()\n",
    "    confidence = probs[label].item()\n",
    "    return {\"answer\": \"True\" if label == 1 else \"False\", \"confidence\": round(confidence, 3)}"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
