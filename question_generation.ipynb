{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5CubXKF-U9S_"
   },
   "source": [
    "# Question Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yn8hnyBU4vwF"
   },
   "outputs": [],
   "source": [
    "!pip install torch datasets pyarrow transformers tokenizers sentencepiece pytorch-lightning textblob nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJOYuUk15ydZ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import copy\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "MAX_SAMPLES = 2000\n",
    "ANSWER_THRESHOLD = 7\n",
    "BATCH_SIZE = 4\n",
    "MAX_EPOCHS = 2\n",
    "LEARNING_RATE = 3e-4\n",
    "\n",
    "pd.options.display.max_rows, pd.options.display.max_columns = 100, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hzSjjWKQ54hq"
   },
   "outputs": [],
   "source": [
    "def create_pandas_dataset(data, answer_threshold=ANSWER_THRESHOLD, verbose=False):\n",
    "  count_long, count_short = 0, 0\n",
    "  result_df = pd.DataFrame(columns=['context', 'answer', 'question'])\n",
    "  for index, val in enumerate(tqdm(data)):\n",
    "      passage = val['context']\n",
    "      question = val['question']\n",
    "      answer = val['answers']['text'][0]\n",
    "      no_of_words = len(answer.split())\n",
    "      if no_of_words >= answer_threshold:\n",
    "          count_long = count_long + 1\n",
    "          continue\n",
    "      else:\n",
    "          result_df.loc[count_short] = [passage] + [answer] + [question]\n",
    "          count_short = count_short + 1\n",
    "  if verbose:\n",
    "    return (result_df, count_long, count_short)\n",
    "  else:\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03QeBDP35-sT"
   },
   "outputs": [],
   "source": [
    "raw_train = load_dataset('squad', split='train')\n",
    "raw_valid = load_dataset('squad', split='validation')\n",
    "print(f\"Total Train Samples:{len(raw_train)} , Total Validation Samples:{len(raw_valid)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8rgwfTB46Tap"
   },
   "outputs": [],
   "source": [
    "df_train, df_validation = create_pandas_dataset(raw_train), create_pandas_dataset(raw_valid)\n",
    "print(f\"\\n Total Train Samples:{df_train.shape} , Total Validation Samples:{df_validation.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mv194_BSQlaR"
   },
   "outputs": [],
   "source": [
    "df_train.to_parquet('train_squad.parquet')\n",
    "df_validation.to_parquet('validation_squad.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_VdUdR1c7iMo"
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    AutoTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1hKYwk8t7mx0"
   },
   "outputs": [],
   "source": [
    "t5_tokenizer = AutoTokenizer.from_pretrained('t5-large', model_max_length=512)\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained('t5-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xnq468XL7p6G"
   },
   "outputs": [],
   "source": [
    "class QuestionGenerationDataset(Dataset):\n",
    "    def __init__(self, tokenizer, filepath, max_len_inp=512, max_len_out=96, max_samples=None):\n",
    "        self.path = filepath\n",
    "\n",
    "        self.passage_column = \"context\"\n",
    "        self.answer = \"answer\"\n",
    "        self.question = \"question\"\n",
    "\n",
    "        self.data = pd.read_parquet(self.path)\n",
    "        if max_samples is not None:\n",
    "            self.data = self.data.iloc[:max_samples, :]\n",
    "\n",
    "        self.max_len_input = max_len_inp\n",
    "        self.max_len_output = max_len_out\n",
    "        self.tokenizer = tokenizer\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "        self._build()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
    "        target_ids = self.targets[index][\"input_ids\"].squeeze()\n",
    "\n",
    "        src_mask = self.inputs[index][\"attention_mask\"].squeeze()\n",
    "        target_mask = self.targets[index][\"attention_mask\"].squeeze()\n",
    "\n",
    "        labels = copy.deepcopy(target_ids)\n",
    "        labels[labels == 0] = -100\n",
    "\n",
    "        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask, \"labels\": labels}\n",
    "\n",
    "    def _build(self):\n",
    "        for rownum, val in tqdm(self.data.iterrows()):\n",
    "            passage, answer, target = val[self.passage_column], val[self.answer], val[self.question]\n",
    "\n",
    "            input_ = f\"context: {passage}  answer: {answer}\"\n",
    "            target = f\"question: {str(target)}\"\n",
    "\n",
    "            tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
    "                [input_], max_length=self.max_len_input, padding='max_length',\n",
    "                truncation=True, return_tensors=\"pt\"\n",
    "            )\n",
    "            tokenized_targets = self.tokenizer.batch_encode_plus(\n",
    "                [target], max_length=self.max_len_output, padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            self.inputs.append(tokenized_inputs)\n",
    "            self.targets.append(tokenized_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bw1kATs872dw"
   },
   "outputs": [],
   "source": [
    "train_path = 'train_squad.parquet'\n",
    "validation_path = 'validation_squad.parquet'\n",
    "train_dataset = QuestionGenerationDataset(t5_tokenizer, train_path, max_samples=MAX_SAMPLES)\n",
    "validation_dataset = QuestionGenerationDataset(t5_tokenizer, validation_path, max_samples=MAX_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Lr6sdb-RD5z"
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch.optim import AdamW\n",
    "\n",
    "class T5Tuner(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, t5model, t5tokenizer, train_data, val_data, batchsize=BATCH_SIZE, lr=LEARNING_RATE):\n",
    "        super().__init__()\n",
    "        self.model = t5model\n",
    "        self.tokenizer = t5tokenizer\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.batch_size = batchsize\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None,\n",
    "                decoder_attention_mask=None,\n",
    "                lm_labels=None):\n",
    "\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            labels=lm_labels,\n",
    "        )\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self.forward(\n",
    "            input_ids=batch[\"source_ids\"],\n",
    "            attention_mask=batch[\"source_mask\"],\n",
    "            decoder_attention_mask=batch['target_mask'],\n",
    "            lm_labels=batch['labels']\n",
    "        )\n",
    "\n",
    "        loss = outputs[0]\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self.forward(\n",
    "            input_ids=batch[\"source_ids\"],\n",
    "            attention_mask=batch[\"source_mask\"],\n",
    "            decoder_attention_mask=batch['target_mask'],\n",
    "            lm_labels=batch['labels']\n",
    "        )\n",
    "\n",
    "        loss = outputs[0]\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, batch_size=self.batch_size,\n",
    "                          num_workers=2)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_data,\n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=2)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=self.lr, eps=1e-8)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2IcZnS20RZza"
   },
   "outputs": [],
   "source": [
    "tuner = T5Tuner(t5_model, t5_tokenizer, train_dataset, validation_dataset)\n",
    "trainer = pl.Trainer(max_epochs=MAX_EPOCHS, accelerator=DEVICE)\n",
    "trainer.fit(tuner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cJECm5cBUQWw"
   },
   "outputs": [],
   "source": [
    "tuner.model.save_pretrained('t5_trained_model')\n",
    "t5_tokenizer.save_pretrained('t5_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XoOtZN8cUT0U"
   },
   "outputs": [],
   "source": [
    "trained_model_path = 't5_trained_model'\n",
    "trained_tokenizer = 't5_tokenizer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g_SSH2G6UZZw"
   },
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(trained_model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(trained_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMwgcH2iWcd7"
   },
   "source": [
    "# POS Tagging for Answer Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "byHuhlYSWxWo"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iSMX6dy0W5h9"
   },
   "outputs": [],
   "source": [
    "def find_possible_answers(context):\n",
    "  possible_answers = []\n",
    "  text = TextBlob(context)\n",
    "  for np in text.noun_phrases:\n",
    "    possible_answers.append(np)\n",
    "  return possible_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Z4SYblQWI2v"
   },
   "source": [
    "# Wh-Question Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mU-9cZKAtbQX"
   },
   "outputs": [],
   "source": [
    "def generate_wh_question(sentence, answer, model, tokenizer):\n",
    "  text = \"context: {} answer: {}\".format(sentence, answer)\n",
    "  max_len = 256\n",
    "  encoding = tokenizer.encode_plus(text, max_length=max_len, padding=False, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "  input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
    "\n",
    "  outs = model.generate(input_ids=input_ids,\n",
    "                                  attention_mask=attention_mask,\n",
    "                                  early_stopping=True,\n",
    "                                  num_beams=5,\n",
    "                                  num_return_sequences=1,\n",
    "                                  no_repeat_ngram_size=2,\n",
    "                                  max_length=72)\n",
    "\n",
    "  dec = [tokenizer.decode(ids, skip_special_tokens=True) for ids in outs]\n",
    "\n",
    "  question = dec[0].replace(\"question:\", \"\")\n",
    "  question = question.strip()\n",
    "  return question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJyyedJQVqyj"
   },
   "source": [
    "# Multiple-Choice Question Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bql9F2dAWFjf"
   },
   "outputs": [],
   "source": [
    "def is_substring(s1, stringlist):\n",
    "    for s2 in stringlist:\n",
    "        if s1 in s2 or s2 in s1:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def mc_is_possible(word_list):\n",
    "  unique_words = []\n",
    "  for word in word_list:\n",
    "    if not is_substring(word, unique_words):\n",
    "      unique_words.append(word)\n",
    "  return len(unique_words) >= 4\n",
    "\n",
    "def generate_answer_choices(correct_answer, answers):\n",
    "  distractors = [a for a in answers if a != correct_answer]\n",
    "  if len(distractors) < 3:\n",
    "    return []\n",
    "\n",
    "  random.shuffle(distractors)\n",
    "  distractors = distractors[:3]\n",
    "\n",
    "  answer_choices = [\"A. \", \"B. \", \"C. \", \"D. \"]\n",
    "  correct_idx = random.randint(0, 3)\n",
    "  answer_choices[correct_idx] += correct_answer\n",
    "\n",
    "  distractor_idx = 0\n",
    "  for i in range(4):\n",
    "    if i != correct_idx:\n",
    "      answer_choices[i] += distractors[distractor_idx]\n",
    "      distractor_idx += 1\n",
    "\n",
    "  return answer_choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3bIXj4ouYrqG"
   },
   "outputs": [],
   "source": [
    "def generate_mc_question(sentence, answer, possible_answers, model, tokenizer):\n",
    "  question = generate_wh_question(sentence, answer, model, tokenizer)\n",
    "  answer_choices = generate_answer_choices(answer, possible_answers)\n",
    "  if len(answer_choices) == 0:\n",
    "    return question\n",
    "  mc_question = question + '\\n' + '\\n'.join(answer_choices)\n",
    "  return mc_question"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
